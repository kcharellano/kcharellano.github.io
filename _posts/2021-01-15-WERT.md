---
title: "WERT: Game + Reinforcement Learning"
date: 2021-01-15T15:34:30-04:00
categories:
  - blog
tags:
  - WERT
  - Q-Learning
---
WERT is a reinforcement learning project created using LibGDX and is inspired by the notorious 2008 web browser game [QWOP](https://en.wikipedia.org/wiki/QWOP). The objective is to move a ragdoll-type player(named Timmy) forward using four actions -- *dubbed w, e, r, t*. What makes this game difficult is the unintuitive nature of how each action affects the players legs. Thus, the goal of this project is to use reinforcement learning to teach an agent how to successfully play the game.

<img src="/assets/gifs/wert_clip.gif" width = "800" height = "500"/>

# In-Depth

The reinforcement learning algorithm used is called q-learning and the goal in q-learning is to find the best action to take given a state and a set of actions. At first the algorthim knows nothing about the world, it only knows the actions that it can perform and the current state that it is in. After performing an action it may receive a positive or negative reward and it will associate that +/- reward with the (state, action) pair that caused it. There is usually also terminal states that normally result in large +/- rewards depending on if the state is a win or lose state. After many iterations of trial and error the algorithm will slowly build up "knowledge" about the world in the form of which actions to take in a given state to gain the most reward. 

The following is a slightly deeper explanation of q-learning in the scope of WERT.

## States
<div style= "text-align: center"><img src="/assets/images/wert_player.png" width = "200" height = "200"/></div>

WERT has two types of states: regular states and terminal states.
A regular state is defined by four angles. Each angle corresponds to one of the angles create by Timmy's legs. That is, two hip angles and two knee angles. These angles are then discretized into bucket ranges instead of actual degree measurements in order to control and limit the amount of possible states. 

A terminal state has nothing do with the angles of Timmy's legs and instead with the position of Timmy's body in the world. There are two terminal states. One is at the leftmost of the screen and the other is at the rightmost of the screen. If Timmy passes over any of these two boundaries the game ends. 

## Actions
There are four possible actions Timmy can take, each is named after a key on the keyboard -- w, e, r, and t. These four actions are responsible for Timmy's movement.

- **Action W:** Rotates the right thigh clockwise and the left thigh counter clockwise
- **Action E:** Rotates the left thigh clockwise and the right thigh counter clockwise
- **Action R:** Rotates the right calf clockwise and the left calf counter clockwise
- **Action T:** Rotates the left calf clockwise and the right calf counter clockwise

## Rewards and Terminal States
WERT uses a simple reward function which is `newXPos - oldXPos` i.e whether Timmy moved forward or backward. If an action causes Timmy to be ahead of where he was before then he gains a positive reward and a negative/no reward otherwise. As mentioned previously, WERT has two terminal states. The leftmost terminal state is a lose state and results in a very large negative reward. The rightmost state is a win state and results in a very large positive reward. Thus, Timmy is incentivized to move forward.

## Hyperparameters
A hyperparameter is fancy name for a variable that controls the learning process. WERT uses three hyperparameters: `discountFactor`, `learningRate` and `randomActionProbability`. These three hyperparameters are usually denoted by `γ`, `α`, and `ε` respectively. All are usually in the range 0 and 1.

- The  `discountFactor` determines the importance of immediate rewards versus future rewards. WERT sets this value to 0.8 which means we care about future rewards(the terminal states) more than immediate rewards.

- The `learningRate` determines how much the algorithm learns after each step. WERT sets this value to 0.2 which means the algorithm learns slowly, over many iterations, which actions consistently lead to good rewards.

- The `randomActionProbability` determines how often the algorithm chooses a random action over a greedy action. WERT sets this value to 0.7 which means to take a random action 30% of the time and a greedy action 70% of the time.

## Tying It All Together
Now you might be thinking to yourself, "Alright Kevin, but *how* does it all tie together?". The learning process can be boiled down to the following steps:

- Choose an action
- Perform the action
- Learn from the action
- Repeat

### Choosing an Action
This is where the `ε` hyperparameter is used and is a fairly simply process Basically, choose a random number between 0 and 1. If the number is less than `ε` then choose a random action otherwise choose a greedy action.
```java
public int chooseAction(Quadruple state) {
  int action = -1;
  if(rand.nextFloat() < eps) {
    // randomly select action between 0 and 3 inclusive
    action = rand.nextInt(qVals.get(state).length);
  }
  else {
    // greedily select best action given the state
    action = argmax(qVals.get(state));
  }
  return action;
}
```

But what is a greedy action? A greedy action in the one that yields the best value. For example, say Timmy is in a state where each action W, E, R, T results in a reward of +1, -2, +3, -1 respectively. Then the greedy action in this state would be R because it results in the highest immediate reward. 

But why choose a random action? If timmy only took greedy actions then he would quickly stick to one and only one sequence of actions. The introduction of randomness allows Timmy to explore paths he wouldn't have otherwise explored. 

### Performing The Action
Performing an action results in a new state. And from this new state we can determine what the reward is for being in that state. As mentioned before, WERT assigns the reward to be the difference between the new position and the old position unless its a terminal state in which a large +/- reward in given instead.

### Learning From The Action
This is where the magic happens and the algortihm *learns*. The function is taking the results it got from perfoming an action and updating the previous state to reflect those results. The following in a code snippet of the learn function in WERT.
```java
public void learn(Quadruple oldState, int action, float reward, Quadruple nextState) {
  float newBestQVal = maxVal(qVals.get(nextState));
  float newVal = reward + discountFactor * newBestQVal;
  float oldVal = qVals.get(oldState)[action];
  qVals.get(oldState)[action] = oldVal + learningRate * (newVal - oldVal);
}
```
You can ignore the type names and focus only on the variable names. First we get the best possible value we can hope to achieve after performing an action. Then we discount that value using `γ` and add it the reward we recieved after performing th eaction. We get the action-value from the last time we performed this action in this state and update it with the new value. The `learningRate` is also used here to minimize how much is *learned*.

## The Result

<img src="/assets/gifs/wert_clip.gif" width="800" height="800"/>

Here is a [link](https://github.com/kcharellano/WERT) to the github repository. If you made it this far, thanks for reading!