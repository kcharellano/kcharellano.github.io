---
title: "WERT: Game + Reinforcement Learning"
date: 2021-01-15T15:34:30-04:00
categories:
  - blog
tags:
  - WERT
  - Q-Learning
  - Reinforcement Learning
---
WERT is a reinforcement learning project created using [LibGDX](https://libgdx.badlogicgames.com/) and inspired by the notorious 2008 web browser game [QWOP](https://en.wikipedia.org/wiki/QWOP). The objective is to move a ragdoll-type player(named Timmy) forward using four actions -- dubbed  *w, e, r, t*. What makes this game challenging is the funky nature of how each action effects Timmy's legs. The goal of this project is to use reinforcement learning to teach Timmy how to move forward by himself.

<img src="/assets/gifs/wert_clip.gif" width = "800" height = "500"/>

# In-Depth

The reinforcement learning algorithm used is called q-learning and the goal in q-learning is to find the best action to take given a state and a set of actions. Initially, the algorthim knows nothing about the world. All it knows is the current state that it is in and the actions it can perform. Performing an action may result in a positive or negative reward and it will then associate that positive or negative reward with the (state, action) pair that caused it. There are also terminal states that normally result in large positive or negative rewards depending on if the state is a win or lose state. After many iterations of trial and error the algorithm will slowly build up "knowledge" about the world in the form of which good and bad actions to take in a given state.

The following is a slightly deeper explanation of q-learning in the scope of WERT.

## States
<div style= "text-align: center"><img src="/assets/images/wert_player.png" width = "200" height = "200"/></div>

WERT has two types of states: regular states and terminal states.
A regular state is defined by four angles. Each angle corresponds to one of the angles created by Timmy's legs. That is, two hip angles and two knee angles. These angles are then discretized into bucket ranges instead of actual degree measurements in order to control and limit the amount of possible states. 

A terminal state has nothing do with the angles of Timmy's legs and instead with the horizontal position of Timmy's body within the world. There are two terminal states. One is at the leftmost side of the screen and the other is at the rightmost side of the screen. If Timmy passes over any of these two boundaries the game ends. 

## Actions
There are four possible actions Timmy can take, each is named after a key on the keyboard -- *w, e, r, and t*. These four actions are responsible for Timmy's movement.

- **Action W:** Rotates the right thigh clockwise and the left thigh counter clockwise.
- **Action E:** Rotates the left thigh clockwise and the right thigh counter clockwise.
- **Action R:** Rotates the right calf counter clockwise and the left calf clockwise.
- **Action T:** Rotates the left calf counter clockwise and the right calf clockwise.

## Rewards
WERT uses a simple reward function `reward = newXPos - oldXPos` i.e whether Timmy moved forward or backward. If an action causes Timmy to be ahead of where he was before then he gains a positive reward and a negative/no reward otherwise. As mentioned previously, WERT has two terminal states. The leftmost terminal state is a lose state and results in a very large negative reward. The rightmost state is a win state and results in a very large positive reward. These rewards incentivize Timmy to move forward instead of backward.

## Hyperparameters
A hyperparameter is fancy name for a variable that controls the learning process. WERT uses three hyperparameters: `discountFactor`, `learningRate` and `randomActionProbability`. These three hyperparameters are usually denoted by `γ`, `α`, and `ε` respectively. All are in the range 0 and 1.

- The  `discountFactor` determines the importance of immediate rewards versus future rewards. WERT sets this value to 0.8 which means we care about future rewards more than immediate rewards.

- The `learningRate` determines how much the algorithm learns after each step. WERT sets this value to 0.2 which means the algorithm learns slowly, over many iterations, which actions consistently lead to good rewards.

- The `randomActionProbability` determines how often the algorithm chooses a random action over a greedy action. WERT sets this value to 0.3 which means to take a random action 30% of the time and a greedy action 70% of the time.

## Tying It All Together
Now you might be thinking to yourself, "Alright Kevin, but *how* does it all tie together?". The learning process can be boiled down to the following steps:

- Choose an action
- Perform the action
- Learn from the action
- Repeat

### Choosing an Action
This is where the `ε` hyperparameter is used and is a fairly straightforward process Basically, choose a random number between 0 and 1. If the number is less than `ε` then choose a random action otherwise choose a greedy action.
```java
public int chooseAction(Quadruple state) {
  int action = -1;
  if(rand.nextFloat() < eps) {
    // randomly select action between 0 and 3 inclusive
    action = rand.nextInt(qVals.get(state).length);
  }
  else {
    // greedily select best action given the state
    action = argmax(qVals.get(state));
  }
  return action;
}
```

But what is a greedy action? A greedy action is one that yields the best reward. For example, say Timmy is in a state where each action W, E, R, T results in a reward of +1, -2, +3, -1 respectively. Then the greedy action in this state would be R because it results in the highest immediate reward. 

But why choose a random action sometimes? If Timmy only took greedy actions then he would quickly stick to one and only one sequence of actions. The introduction of randomness allows Timmy to explore paths he wouldn't have otherwise attempted to explore. 

### Performing The Action
Performing an action results in a new state. And from this new state we can determine what the reward is for transitioning to that state. As mentioned before, WERT assigns the reward to be the difference between the new position and the old position unless the new state is a terminal state in which case a large positive or negative reward is given instead.

### Learning From The Action
This is where the magic happens and where the algortihm *learns*. The following function takes the results it got from perfoming an action and updates its "knowledge" to reflect those results. The following code snippet is from the learn function used in WERT.
```java
public void learn(Quadruple oldState, int action, float reward, Quadruple nextState) {
  float newBestQVal = maxVal(qVals.get(nextState));
  float newVal = reward + discountFactor * newBestQVal;
  float oldVal = qVals.get(oldState)[action];
  qVals.get(oldState)[action] = oldVal + learningRate * (newVal - oldVal);
}
```
You can ignore the type names and focus only on the variable names. First we get the best possible value we can hope to achieve after performing an action. Then we discount that value using `γ` and add it the reward we recieved after performing the action. We get the action-value from the last time we performed this action in this state and update it with the new action-value. The `learningRate` is also used here to minimize how much is *learned* during each update.

## The Result

<img src="/assets/gifs/wert_clip.gif" width="800" height="800"/>

Although my initial goal was to have Timmy learn how to walk like a person, it seems he found an easier way to move by inching his way forward using one of his knees only.

Here is a [link](https://github.com/kcharellano/WERT) to the github repository. If you made it this far, thanks for reading!
